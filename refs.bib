
@Article{MBC22,
AUTHOR = {Mukhiddinov, Mukhriddin and Abdusalomov, Akmalbek Bobomirzaevich and Cho, Jinsoo},
TITLE = {Automatic Fire Detection and Notification System Based on Improved YOLOv4 for the Blind and Visually Impaired},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {9},
ARTICLE-NUMBER = {3307},
URL = {https://www.mdpi.com/1424-8220/22/9/3307},
PubMedID = {35590996},
ISSN = {1424-8220},
ABSTRACT = {The growing aging population suffers from high levels of vision and cognitive impairment, often resulting in a loss of independence. Such individuals must perform crucial everyday tasks such as cooking and heating with systems and devices designed for visually unimpaired individuals, which do not take into account the needs of persons with visual and cognitive impairment. Thus, the visually impaired persons using them run risks related to smoke and fire. In this paper, we propose a vision-based fire detection and notification system using smart glasses and deep learning models for blind and visually impaired (BVI) people. The system enables early detection of fires in indoor environments. To perform real-time fire detection and notification, the proposed system uses image brightness and a new convolutional neural network employing an improved YOLOv4 model with a convolutional block attention module. The h-swish activation function is used to reduce the running time and increase the robustness of YOLOv4. We adapt our previously developed smart glasses system to capture images and inform BVI people about fires and other surrounding objects through auditory messages. We create a large fire image dataset with indoor fire scenes to accurately detect fires. Furthermore, we develop an object mapping approach to provide BVI people with complete information about surrounding objects and to differentiate between hazardous and nonhazardous fires. The proposed system shows an improvement over other well-known approaches in all fire detection metrics such as precision, recall, and average precision.},
DOI = {10.3390/s22093307}
}

@article{EPF14,
  author       = {David Eigen and
                  Christian Puhrsch and
                  Rob Fergus},
  title        = {Depth Map Prediction from a Single Image using a Multi-Scale Deep
                  Network},
  journal      = {CoRR},
  volume       = {abs/1406.2283},
  year         = {2014},
  url          = {http://arxiv.org/abs/1406.2283},
  eprinttype    = {arXiv},
  eprint       = {1406.2283},
  timestamp    = {Mon, 13 Aug 2018 16:47:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/EigenPF14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{LRS23,
   title={Self-Supervised Monocular Depth Estimation With Self-Reference Distillation and Disparity Offset Refinement},
   volume={33},
   ISSN={1558-2205},
   url={http://dx.doi.org/10.1109/TCSVT.2023.3275584},
   DOI={10.1109/tcsvt.2023.3275584},
   number={12},
   journal={IEEE Transactions on Circuits and Systems for Video Technology},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Liu, Zhong and Li, Ran and Shao, Shuwei and Wu, Xingming and Chen, Weihai},
   year={2023},
   month=dec, pages={7565–7577} }

@inproceedings{10.1145/3613904.3642719,
author = {Puerta, Eduardo and Crnovrsanin, Tarik and South, Laura and Dunne, Cody},
title = {The Effect of Orientation on the Readability and Comfort of 3D-Printed Braille},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642719},
doi = {10.1145/3613904.3642719},
abstract = {Fused Deposition Modeling (FDM) is a low-cost method of 3D printing that involves stacking horizontal layers of plastic. FDM is used to produce tactile graphics and interfaces for people with visual impairments. Unfortunately, the print orientation can alter the structure and quality of braille and text. The difference between printing braille vertically and horizontally has been documented. However, we found no comprehensive study of these angles or the angles in between, nor any study providing a quantitative and qualitative user evaluation. We conducted two mixed-methods studies to evaluate the performance of braille printed at different angles. We measured reading time and subjective preference and performed a thematic analysis of participants’ responses. Our participants were faster using and preferred 75° and vertical braille over horizontal braille. These results provide makers with guidelines for creating models with readable 3D-printed braille.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {346},
numpages = {15},
keywords = {3D Printing, Braille, Visual Accessibility},
location = {Honolulu, HI, USA},
series = {CHI '24}
}